{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import requests  ## for getting data from a server GET\n",
    "import re  \n",
    "import pandas as pd    \n",
    "from pandas import DataFrame\n",
    "import os\n",
    "import json\n",
    "\n",
    "## To tokenize and vectorize text type data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://news.google.com/rss/search?q=%22hurricane%22+AND+%22inequality%22&hl=en-US&gl=US&ceid=US:en',\n",
    "        'https://news.google.com/rss/search?q=hurricane%20-inequality&hl=en-US&gl=US&ceid=US%3Aen']\n",
    "\n",
    "topics = ['hurricane','inequality']\n",
    "filename = \"NewsHeadlines.csv\"\n",
    "MyFile = open(filename, \"w\")  # 'w' -> write to new\n",
    "\n",
    "WriteThis = \"Label,Headline\\n\"\n",
    "MyFile.write(WriteThis)\n",
    "MyFile.close()\n",
    "\n",
    "raw_data = open(\"raw_data.txt\", \"w\")  # 'w' -> write to new\n",
    "raw_data.close()\n",
    "\n",
    "MyFILE=open(filename, \"a\") # \"a\" for append to add stuff\n",
    "raw_data = open(\"raw_data.txt\", \"a\")  # 'w' -> write to new\n",
    "\n",
    "labels = ['inequality','not inequality']\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "\n",
    "    feed = feedparser.parse(url)\n",
    "\n",
    "\n",
    "    raw_data.write(json.dumps(feed.entries))\n",
    "\n",
    "    for items in feed.entries:\n",
    "        label= labels[i]\n",
    "\n",
    "        # print(items, \"\\n\\n\\n\")\n",
    "                    \n",
    "        #Author=items[\"author\"]\n",
    "        #Author=str(Author)\n",
    "        #Author=Author.replace(',', '')\n",
    "        \n",
    "        # ## SOURCE\n",
    "        # Source=items[\"source\"][\"name\"]\n",
    "        # # print(Source)\n",
    "        \n",
    "        # ## DATE\n",
    "        # Date=items[\"published\"]\n",
    "        # NewDate=Date.split(\"T\")\n",
    "        # Date=NewDate[0]\n",
    "        # # print(Date)\n",
    "        \n",
    "        ## TITLE\n",
    "        #  - Replace punctuation with space\n",
    "        #  - Accept one or more copies of punctuation         \n",
    "        #  - plus zero or more copies of a space\n",
    "        #  - and replace it with a single space\n",
    "        Title=items[\"title\"].split('-')[:-1]\n",
    "        Title=str(Title)\n",
    "        Title=re.sub(r'[,.;@#?!&$\\-\\']+', ' ', str(Title), flags=re.IGNORECASE)\n",
    "        Title=re.sub(' +', ' ', str(Title), flags=re.IGNORECASE)\n",
    "        Title=re.sub(r'\\\"', ' ', str(Title), flags=re.IGNORECASE)\n",
    "        Title=re.sub(r'[^a-zA-Z]', \" \", str(Title), flags=re.VERBOSE)\n",
    "        Title=Title.replace(',', '')\n",
    "        Title=' '.join(Title.split())\n",
    "        Title=re.sub(\"\\n|\\r\", \"\", Title)\n",
    "        # print(Title)\n",
    "\n",
    "        # ## HEADLINE\n",
    "        # Headline=items[\"title_detail\"][\"value\"]\n",
    "        # Headline=str(Headline)\n",
    "        # Headline=re.sub(r'[,.;@#?!&$\\-\\']+', ' ', Headline, flags=re.IGNORECASE)\n",
    "        # Headline=re.sub(' +', ' ', Headline, flags=re.IGNORECASE)\n",
    "        # Headline=re.sub(r'\\\"', ' ', Headline, flags=re.IGNORECASE)\n",
    "        # Headline=re.sub(r'[^a-zA-Z]', \" \", Headline, flags=re.VERBOSE)\n",
    "        # Headline=Headline.replace(',', '') # commas are bad for csv\n",
    "        # Headline=' '.join(Headline.split())\n",
    "        # Headline=re.sub(\"\\n|\\r\", \"\", Headline)\n",
    "        \n",
    "        ### AS AN OPTION - remove words of a given length............\n",
    "        ### Headline = ' '.join([wd for wd in Headline.split() if len(wd)>3])\n",
    "        \n",
    "        WriteThis=str(label)+\",\" + str(Title) + \"\\n\"\n",
    "        # print(WriteThis)\n",
    "        \n",
    "        MyFILE.write(WriteThis)\n",
    "\n",
    "## CLOSE THE FILES\n",
    "raw_data.close()\n",
    "MyFILE.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "##\n",
    "## Tokenize and Vectorize the Headlines\n",
    "##\n",
    "#############################################\n",
    "\n",
    "df_raw =pd.read_csv(filename)\n",
    "\n",
    "df_cleaned = df_raw.copy()    \n",
    "\n",
    "## REMOVE any rows with NaN in them\n",
    "df_cleaned = df_cleaned.dropna()\n",
    "\n",
    "HeadlineLIST=[]\n",
    "LabelLIST=[]\n",
    "for nexthead, nextlabel in zip(df_cleaned[\"Headline\"], df_cleaned[\"Label\"]):\n",
    "    HeadlineLIST.append(nexthead)\n",
    "    LabelLIST.append(nextlabel)\n",
    "    \n",
    "    \n",
    "NewHeadlineLIST=[]\n",
    "for element in HeadlineLIST:\n",
    "    AllWords=element.split(\" \")\n",
    "    \n",
    "    ## Now remove words that are in your topics\n",
    "    NewWordsList=[]\n",
    "    for word in AllWords:\n",
    "        word=word.lower()\n",
    "        if word in topics:\n",
    "            pass\n",
    "        else:\n",
    "            NewWordsList.append(word)\n",
    "            \n",
    "    ##turn back to string\n",
    "    NewWords=\" \".join(NewWordsList)\n",
    "    NewHeadlineLIST.append(NewWords)\n",
    "    \n",
    "HeadlineLIST=NewHeadlineLIST\n",
    "\n",
    "### Vectorize\n",
    "## Instantiate your CV\n",
    "MyCountV=CountVectorizer(\n",
    "        input=\"content\",  ## because we have a csv file\n",
    "        lowercase=True, \n",
    "        stop_words = \"english\",\n",
    "        max_features=100\n",
    "        )\n",
    "\n",
    "## Use your CV \n",
    "MyDTM = MyCountV.fit_transform(HeadlineLIST)  # create a sparse matrix\n",
    "\n",
    "ColumnNames=MyCountV.get_feature_names_out()\n",
    "\n",
    "\n",
    "## Build the data frame\n",
    "MyDTM_DF=pd.DataFrame(MyDTM.toarray(),columns=ColumnNames)\n",
    "\n",
    "## Convert the labels from list to df\n",
    "Labels_DF = pd.DataFrame(LabelLIST,columns=['Label'])\n",
    "\n",
    "##Save original DF - without the lables\n",
    "My_Orig_DF=MyDTM_DF\n",
    "\n",
    "## Now - let's create a complete and labeled\n",
    "## dataframe:\n",
    "dfs = [Labels_DF, MyDTM_DF]\n",
    "\n",
    "Final_News_DF_Labeled = pd.concat(dfs,axis=1, join='inner')\n",
    "\n",
    "# Save the vecotrized data to the CleanData Folder\n",
    "Final_News_DF_Labeled.to_csv(f\"NewsHeadlines_vectorized_100.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
