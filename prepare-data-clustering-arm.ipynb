{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "  \n",
    "df1 = pd.read_csv('SVI_2020_US_county.csv')\n",
    "df2 = pd.read_csv('NRI_Table_Counties/NRI_Table_Counties.csv') \n",
    "  \n",
    "# merging df1 and df2 by ID \n",
    "# i.e. the rows with common ID's get merged \n",
    "# with all the ID's of right dataframe i.e. df2 \n",
    "# and NaN values for df1 columns where ID do not match \n",
    "df = pd.merge(df1, df2, on=\"FIPS\", how=\"right\") \n",
    "df_cleaned = df[['ST','STATE_x','STCNTY', 'COUNTY_x','FIPS',\\\n",
    "                 'EP_POV150','EP_HBURD','EP_DISABL','EP_SNGPNT','EP_AGE17','EP_AGE65',\\\n",
    "                 'EP_MINRTY','EP_AFAM','EP_HISP','EP_ASIAN','EP_AIAN','EP_NHPI',\\\n",
    "                 'RISK_VALUE','EAL_VALT',\n",
    "                 'RISK_SCORE','EAL_SCORE','CFLD_RISKS','DRGT_RISKS','HWAV_RISKS',\\\n",
    "                 'HRCN_RISKS','RFLD_RISKS','WFIR_RISKS',\\\n",
    "                    'CFLD_RISKV','HWAV_RISKV',]]\n",
    "\n",
    "df_cleaned = df_cleaned[df_cleaned['STATE_x'].notna()]\n",
    "df_cleaned.fillna(0, inplace=True)\n",
    "df_cleaned.head()\n",
    "\n",
    "# df_cleaned_2 = df_cleaned[['EP_POV150','EP_HBURD','EP_AFAM','EP_HISP','RISK_VALUE']]\n",
    "df_cleaned.to_csv('nri-svi-cleaned.csv', sep=',', index=False, encoding='utf-8')\n",
    "# df_cleaned_2.to_csv('nri-svi-cleaned-2.csv', sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframes as df1 and df2 \n",
    "strong2 = pd.read_csv('STRONG-II/STRONGII_nonTX.csv')\n",
    "strong1 = pd.read_csv('STRONG-I/STRONG_Version_2_Updated_2021-12-01/STRONG_Version_2_Updated_2021-12-01.csv') \n",
    "\n",
    "\n",
    "# merging df1 and df2 by ID \n",
    "# i.e. the rows with common ID's get merged \n",
    "# with all the ID's of right dataframe i.e. df2 \n",
    "# and NaN values for df1 columns where ID do not match \n",
    "strong = pd.merge(strong1, strong2, on=\"respid\", how=\"right\") \n",
    "# print(strong.describe())\n",
    "strong_cleaned = strong[['h2', 'storm3_1','storm3_2','storm3_3','storm3_4','storm3_5',\\\n",
    "                         'storm3a_1','storm3a_1','storm3a_2','storm3a_3','storm3a_4','storm3a_5',\\\n",
    "                 'danger1','danger2','danger3','danger4','danger5',\\\n",
    "                 'shelter1_1','shelter1_2','shelter1_3','shelter1_4','shelter1_5','shelter1_6',\\\n",
    "                 'dam1','dam6','help1','rec1','rec2','rec4',\\\n",
    "                 'hurr1','hurr2','hurr3','hurr4','hurr5','hurr6','hurr7','hurr8',\\\n",
    "                 'job1_y','job1a','job1b','job1c','job2',\\\n",
    "                 'sw1','sw2','sw3','sw4','sw5',\\\n",
    "                 'ph1_y','ph2_y','audit1_y','audit2_y','audit3_y',\\\n",
    "                 'eff1','eff2','eff3','eff4','eff5','eff6',\\\n",
    "                 'phq1_y','phq2_y','phq3','phq4','phq5','phq6','phq7','phq8',\\\n",
    "                 'pcl1','pcl2','pcl3','pcl4','pcl5','pcl6','pcl7','pcl8','pcl9','pcl10',\\\n",
    "                 'pcl11','pcl12','pcl13','pcl14','pcl15','pcl16','pcl17','pcl18','pcl19','pcl20',\\\n",
    "                 'sex','age_y','income','same_respondent','hispanic','white','black','aian','asian','pacific']]\n",
    "\n",
    "\n",
    "strong_trans = pd.DataFrame(columns = ['injured','home_damage','rec_household','rec_community','adversity_score',\\\n",
    "                                       'lost_job','eff_score','depression_score','ptsd_score',\\\n",
    "                                    'sex','age','income','white','black','aian','asian','hispanic','pacific'])\n",
    "\n",
    "\n",
    "strong_cleaned = strong_cleaned[strong_cleaned['storm3_1'].notna()]\n",
    "strong_cleaned = strong_cleaned.loc[(strong_cleaned[['same_respondent']] != 0).all(axis=1)]\n",
    "\n",
    "strong_cleaned['storm_living'] = strong_cleaned[['storm3_1','storm3_2','storm3_3','storm3_4','storm3_5']].sum(axis='columns')\n",
    "strong_cleaned['storm_present'] = strong_cleaned[['storm3a_1','storm3a_2','storm3a_3','storm3a_4','storm3a_5']].sum(axis='columns')\n",
    "strong_cleaned['storm_living_bin'] =  (strong_cleaned['storm_living']>0).astype(int)\n",
    "strong_cleaned['storm_present_bin'] =  (strong_cleaned['storm_present']>0).astype(int)\n",
    "# strong_cleaned['storm_affected'] = strong_cleaned[['storm_living','storm_present']].sum(axis='columns')\n",
    "strong_cleaned = strong_cleaned.loc[(strong_cleaned[['storm_living','storm_present']] != 0).all(axis=1)]\n",
    "\n",
    "\n",
    "# calculate adversity score by summing over relevant questions and bin into low medium high\n",
    "strong_cleaned['adversity_score'] = strong_cleaned[['hurr1','hurr2','hurr3','hurr4','hurr5','hurr6','hurr7','hurr8']].sum(axis='columns')\n",
    "strong_cleaned['adversity_score'][strong_cleaned['adversity_score'] > 200] = np.nan\n",
    "adversity_bins = [0,1,3,strong_cleaned['adversity_score'].max()][1:]\n",
    "labels=['medium adversity', 'high adversity']\n",
    "strong_trans['adversity_score'] = pd.cut(strong_cleaned['adversity_score'], adversity_bins,labels=labels,include_lowest=True)\n",
    "# print(strong_cleaned['adversity_score'].min())\n",
    "# print(strong_cleaned['adversity_score'].max())\n",
    "# print(adversity_bins)\n",
    "\n",
    "# calculate efficacy score by summing over relevant questions and bin into low medium high\n",
    "strong_cleaned['eff_score'] = strong_cleaned[['eff1','eff2','eff3','eff4','eff5','eff6']].sum(axis='columns')\n",
    "strong_cleaned['eff_score'][strong_cleaned['eff_score'] > 200] = np.nan\n",
    "eff_bins = np.linspace(strong_cleaned['eff_score'].min(),strong_cleaned['eff_score'].max(),4)[0:-1]\n",
    "labels=['low efficacy', 'medium efficacy']\n",
    "strong_trans['eff_score'] = pd.cut(strong_cleaned['eff_score'], eff_bins,labels=labels,include_lowest=True)\n",
    "print(strong_cleaned['eff_score'].min())\n",
    "print(strong_cleaned['eff_score'].max())\n",
    "print(eff_bins)\n",
    "\n",
    "# calculate depression score by summing over relevant questions and bin into low medium high\n",
    "strong_cleaned['depression_score'] = strong_cleaned[['phq1_y','phq2_y','phq3','phq4','phq5','phq6','phq7','phq8']].sum(axis='columns')\n",
    "strong_cleaned['depression_score'][strong_cleaned['depression_score'] > 200] = np.nan\n",
    "depression_bins = np.linspace(strong_cleaned['depression_score'].min(),strong_cleaned['depression_score'].max(),4)[1:]\n",
    "labels=['medium depression', 'high depression']\n",
    "strong_trans['depression_score'] = pd.cut(strong_cleaned['depression_score'], depression_bins,labels=labels,include_lowest=True)\n",
    "# print(strong_cleaned['depression_score'].min())\n",
    "# print(strong_cleaned['depression_score'].max())\n",
    "# print(depression_bins)\n",
    "\n",
    "# calculate ptsd score by summing over relevant questions and bin into low medium high\n",
    "strong_cleaned['ptsd_score'] = strong_cleaned[['pcl1','pcl2','pcl3','pcl4','pcl5','pcl6','pcl7','pcl8','pcl9','pcl10',\\\n",
    "                 'pcl11','pcl12','pcl13','pcl14','pcl15','pcl16','pcl17','pcl18','pcl19','pcl20']].sum(axis='columns')\n",
    "strong_cleaned['ptsd_score'][strong_cleaned['ptsd_score'] > 200] = np.nan\n",
    "ptsd_bins = np.linspace(strong_cleaned['ptsd_score'].min(),strong_cleaned['ptsd_score'].max(),4)[1:]\n",
    "labels=['medium ptsd', 'high ptsd']\n",
    "strong_trans['ptsd_score'] = pd.cut(strong_cleaned['ptsd_score'], ptsd_bins,labels=labels,include_lowest=True)\n",
    "# print(strong_cleaned['ptsd_score'].min())\n",
    "# print(strong_cleaned['ptsd_score'].max())\n",
    "# print(ptsd_bins)\n",
    "\n",
    "# mapping ints to descriptions\n",
    "# d_housing = {1:'one-family house',2:'two-family house',3:'apartment',4:'mobile home',5:'row house'}\n",
    "# strong_trans['housing'] = strong_cleaned['h2'].map(d_housing)\n",
    "\n",
    "# d_storm_living = {0:'not_storm_living',1:'storm_living'}\n",
    "# d_storm_present = {0:'not_storm_present',1:'storm_present'}\n",
    "# strong_trans['storm_living'] = strong_cleaned['storm_living_bin'].map(d_storm_living)\n",
    "# strong_trans['storm_present'] = strong_cleaned['storm_present_bin'].map(d_storm_present)\n",
    "\n",
    "\n",
    "d_injured = {1:'minor injury',2:'major injury'}\n",
    "strong_trans['injured'] = strong_cleaned['danger1'].map(d_injured)\n",
    "\n",
    "d_damage = {1: 'damaged, livable', 2: 'damaged, not livable', 3: 'destroyed'}\n",
    "strong_trans['home_damage'] = strong_cleaned['dam1'].map(d_damage)\n",
    "\n",
    "d_lost_job = {2: \"lost job\"}\n",
    "strong_trans['lost_job'] = strong_cleaned['job1_y'].map(d_lost_job)\n",
    "\n",
    "d_rec_household = {0: \"no household recovery\"}\n",
    "strong_trans['rec_household'] = strong_cleaned['rec1'].map(d_rec_household)\n",
    "\n",
    "d_rec_community = {0: \"no community recovery\"}\n",
    "strong_trans['rec_community'] = strong_cleaned['rec2'].map(d_rec_community)\n",
    "\n",
    "d_sex = {1:'male',2:'female',3:'trans male',4:'trans female',5:'gnc'}\n",
    "strong_trans['sex'] = strong_cleaned['sex'].map(d_sex)\n",
    "\n",
    "d_age = {1:'18-25',2:'26-50',3:'51-75',4:'over 75'}\n",
    "strong_trans['age'] = strong_cleaned['age_y'].map(d_age)\n",
    "\n",
    "d_income = {1:'under 10k',2:'10k-20k',3:'20k-30k',4:'30k-40k',5:'40k-50k'}\n",
    "strong_trans['income'] = strong_cleaned['income'].map(d_income)\n",
    "\n",
    "races_ethnicities = ['white','black','aian','asian','hispanic','pacific']\n",
    "\n",
    "for race in races_ethnicities:\n",
    "        d_race = {1:race}\n",
    "        strong_trans[race] = strong_cleaned[race].map(d_race)\n",
    "\n",
    "\n",
    "strong_trans.to_csv('strong_transaction.csv', sep=',', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
